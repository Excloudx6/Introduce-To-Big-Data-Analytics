sapply(College[, -c(1, 18)], function(x) cor.test(x, College$Grad.Rate, method = "spearman")$p.value)
wilcox.test(Grad.Rate ~ Private, data = College)$p.value
## 1.2
# set ramdom seed 10
set.seed(10)
# split the data
n = nrow(College)
spl = 1:n %in% sample(1:n, round(0.7 * n))
TR = College[spl, ]
TS = College[!spl, ]
# split the data
n = nrow(College)
spl = 1:n %in% sample(1:n, round(0.7 * n))
TR = College[spl, ]
TS = College[!spl, ]
# fit a linear model
model = lm(Grad.Rate ~ . -Books, data = College)
# fit a linear model
model = lm(Grad.Rate ~ . -Books, data = TR)
TR_pred = predict(model, TR)
TS_pred = predict(model, TS)
# create a function RMSE
rmse = function(y_true, y_pred){
return (sqrt(mean((y_true - y_pred) ^ 2)))
}
rmse(TR$Grad.Rate ,TR_pred) # 12.57997
rmse(TS$Grad.Rate ,TS_pred) # 12.65215
# fit a linear model
model = lm(Grad.Rate ~ . -Books, data = College)
TR_pred = predict(model, TR)
TS_pred = predict(model, TS)
# create a function RMSE
rmse = function(y_true, y_pred){
return (sqrt(mean((y_true - y_pred) ^ 2)))
}
rmse(TR$Grad.Rate ,TR_pred) # 12.57997
rmse(TS$Grad.Rate ,TS_pred) # 12.65215
rm(list = ls())
pacman::p_load(tidyverse, data.table, ISLR, randomForest, pROC, Metrics, FNN)
# Load the built-in dataset "College"
data("College")
?College
sum(is.na(College))
## 1.1 Do a series of bivariate analyses on the "Grad.Rate" vs. the rest of variables.
shapiro.test(College$Grad.Rate) # p-value < 0.05, "Grad.Rate" is not normal
# "Books" p-value > 0.05, "Books" is not associated with "Grad.Rate
# "Top25perc" p-value < 0.05, "Top25perc" is associated with "Grad.Rate
sapply(College[, -c(1, 18)], function(x) cor.test(x, College$Grad.Rate, method = "spearman")$p.value)
wilcox.test(Grad.Rate ~ Private, data = College)$p.value
## 1.2
# set ramdom seed 10
set.seed(10)
# split the data
n = nrow(College)
spl = 1:n %in% sample(1:n, round(0.7 * n))
TR = College[spl, ]
TS = College[!spl, ]
# fit a linear model
model = lm(Grad.Rate ~ . -Books, data = College)
TR_pred = predict(model, TR)
TS_pred = predict(model, TS)
# create a function RMSE
rmse = function(y_true, y_pred){
return (sqrt(mean((y_true - y_pred) ^ 2)))
}
rmse(TR$Grad.Rate ,TR_pred) # 12.57997
rmse(TS$Grad.Rate ,TS_pred) # 12.65215
## 1.3 non-linear relationship between "Grad.Rate" and "Top10perc","Expend", "perc.alumni"
summary(lm(Grad.Rate ~ poly(Top10perc, 5), data = College)) # There is no non-linear relationship between "Grad.Rate" and "Top10perc"
summary(lm(Grad.Rate ~ poly(Expend, 5), data = College)) # There is a non-linear relationship between "Grad.Rate" and "Expend"
summary(lm(Grad.Rate ~ poly(perc.alumni, 5), data = College)) # There is a non-linear relationship between "Grad.Rate" and "perc.alumni"
## 1.4
# Ans : 新模型的表現與之前的差不多
model = lm(Grad.Rate ~ . -Books + poly(Expend, 2) + poly(perc.alumni, 2), data = College)
TR_pred = predict(model, TR)
TS_pred = predict(model, TS)
rmse(TR$Grad.Rate ,TR_pred) # 12.38808
rmse(TS$Grad.Rate ,TS_pred) # 12.63151
col_names = c("class", "cap_shape", "cap_surface", "cap_color", "bruises", "odor", "gill_attachment", "gill_spacing", "gill_size", "gill_color", "stalk_shape", "stalk_root",
"stalk_surface_above_ring","stalk_surface_below_ring", "stalk_color_above_ring",
"stalk_color_below_ring", "veil_type", "veil_color", "ring_number", "ring_type",
"spore_print_color", "population", "habitat")
rm(list = ls())
pacman::p_load(tidyverse, data.table, ISLR, randomForest, pROC, Metrics, FNN)
# Load the built-in dataset "College"
data("College")
?College
sum(is.na(College))
## 1.1 Do a series of bivariate analyses on the "Grad.Rate" vs. the rest of variables.
shapiro.test(College$Grad.Rate) # p-value < 0.05, "Grad.Rate" is not normal
# "Books" p-value > 0.05, "Books" is not associated with "Grad.Rate
# "Top25perc" p-value < 0.05, "Top25perc" is associated with "Grad.Rate
sapply(College[, -c(1, 18)], function(x) cor.test(x, College$Grad.Rate, method = "spearman")$p.value)
wilcox.test(Grad.Rate ~ Private, data = College)$p.value
## 1.2
# set ramdom seed 10
set.seed(10)
# split the data
n = nrow(College)
spl = 1:n %in% sample(1:n, round(0.7 * n))
TR = College[spl, ]
TS = College[!spl, ]
# fit a linear model
model = lm(Grad.Rate ~ . -Books, data = TR)
TR_pred = predict(model, TR)
TS_pred = predict(model, TS)
# create a function RMSE
rmse = function(y_true, y_pred){
return (sqrt(mean((y_true - y_pred) ^ 2)))
}
rmse(TR$Grad.Rate ,TR_pred) # 12.57997
rmse(TR$Grad.Rate ,TR_pred) # 12.5337
rmse(TS$Grad.Rate ,TS_pred) # 12.65215
rmse(TS$Grad.Rate ,TS_pred) # 12.90766
## 1.3 non-linear relationship between "Grad.Rate" and "Top10perc","Expend", "perc.alumni"
summary(lm(Grad.Rate ~ poly(Top10perc, 5), data = College)) # There is no non-linear relationship between "Grad.Rate" and "Top10perc"
summary(lm(Grad.Rate ~ poly(Expend, 5), data = College)) # There is a non-linear relationship between "Grad.Rate" and "Expend"
summary(lm(Grad.Rate ~ poly(perc.alumni, 5), data = College)) # There is a non-linear relationship between "Grad.Rate" and "perc.alumni"
## 1.4
# Ans : 新模型的表現與之前的差不多
model = lm(Grad.Rate ~ . -Books + poly(Expend, 2) + poly(perc.alumni, 2), data = College)
## 1.4
# Ans : 新模型的表現與之前的差不多
model = lm(Grad.Rate ~ . -Books + poly(Expend, 2) + poly(perc.alumni, 2), data = TR)
TR_pred = predict(model, TR)
TS_pred = predict(model, TS)
rmse(TR$Grad.Rate ,TR_pred) # 12.38808
rmse(TR$Grad.Rate ,TR_pred) # 12.32931
rmse(TS$Grad.Rate ,TS_pred) # 12.63151
rmse(TS$Grad.Rate ,TS_pred) # 12.95821
col_names = c("class", "cap_shape", "cap_surface", "cap_color", "bruises", "odor", "gill_attachment", "gill_spacing", "gill_size", "gill_color", "stalk_shape", "stalk_root",
"stalk_surface_above_ring","stalk_surface_below_ring", "stalk_color_above_ring",
"stalk_color_below_ring", "veil_type", "veil_color", "ring_number", "ring_type",
"spore_print_color", "population", "habitat")
## 2.1
# Convert the variable type from character to factor
mushrooms = fread("./mushrooms.csv", fill = T, stringsAsFactors = T)
# change the variable names
names(mushrooms) = col_names
## 2.2 remove those records with missing "stalk_root"
mushrooms = subset(mushrooms, stalk_root != "?")
## 2.3
# fit a simple logistic regression model
model = glm(class ~ bruises, data = mushrooms, family = binomial)
# computer the predicted probability of class = poisonous
summary(model)$coef # 0.8121993 -2.2906666
exp(0.8121993 + (-2.2906666) * 1)/ (1 + exp(0.8121993 + (-2.290666) * 1)) # bruises : 0.185659
exp(0.8121993 + (-2.2906666) * 0)/ (1 + exp(0.8121993 + (-2.290666) * 0)) # no bruises : 0.692578
## 2.4
# Create a crosstab "bruises by class" and calculate the percentages
props = prop.table(xtabs(~ bruises + class, data = mushrooms), 1)
# compute odds ratio
# 勝算比 (OR) 可被解釋為發生率比
OR = props[, 2] / props[, 1]
OR[2] / OR[1] # odds ratio
## 2.5
# Split the data into training (70%) and testing (30%) datasets with set.seed(0)
set.seed(0)
n = nrow(mushrooms)
spl = 1:n %in% sample(1:n, round(0.7 * n))
TR = mushrooms[spl, ]
TS = mushrooms[!spl, ]
# top-3 important variables from random forests variable importance ranking
class.rf = randomForest(class ~ ., data = TR)
varImpPlot(class.rf) # top-3 important variables : odor、spore_print_color、gill_color
# from slides unit 5 page 77
# Get k-fold CV confusion matrix for Logistic Regression model
# f: formula, d: data, k: number of folds, cutoff: cutoff point 0-1
k_fold_CV_logit = function(f, d, k, cutoff){
numOfRec = nrow(d) # number of observations
reponse_var = all.vars(f)[1] # name of the reponse variable
# k indices used to split data into k parts
sample_idx_k = rep(sample(1:k),round(numOfRec / k) + 1)[1:numOfRec]
# k models for k subsets of data
k_fits = Map( function(x) glm(f, d[sample_idx_k != x, ], family = "binomial"), 1:k)
# Predicted & actual classes for each hold-out subset
predActualClass = Map(function(x){
predictedProb = predict(k_fits[[x]], d[sample_idx_k == x,], type = "response")
predictedClass = ifelse(predictedProb > cutoff, 1, 0)
return(data.frame("predictedClass" =  predictedClass, "actualClass" = d[sample_idx_k == x, reponse_var] ) )
}, 1:k)
# A data frame with all predicted & actual classes
output_DF = Reduce(function(x, y) rbind(x, y), predActualClass)
output_DF$predictedClass = factor(output_DF$predictedClass, levels=c(0,1),labels = c("No", "Yes"))
return( table(output_DF$predictedClass, output_DF$actualClass))
}
TR = as.data.frame(TR)
# accuracy of these three models : 0.9837545、0.8691336、0.8027076
Map(function(x) sum(diag(k_fold_CV_logit(x, TR, 10, 0.5))) / nrow(TR),
list(class ~ odor,  class ~ spore_print_color, class ~ gill_color))
## 2.6
model = glm(class ~ odor, data = TR, family = binomial)
TR_pred = predict(model, TR, type = "response")
TS_pred = predict(model, TS, type = "response")
plot.roc(roc(TR$class, TR_pred), print.thres = "best", print.auc = T) # AUC : 0.986
plot.roc(roc(TS$class, TS_pred), print.thres = "best", print.auc = T) # AUC : 0.989
# load dataset "Carseats"
rm(list = ls())
data("Carseats")
sum(is.na(Carseats))
## 3.1
# Split the dataset into training (70%) and testing sets (30%) with random seed set.seed(1)
set.seed(1)
n = nrow(Carseats)
spl = 1:n %in% sample(1:n, round(0.7 * n))
CS_train = Carseats[spl, ]
CS_test = Carseats[!spl, ]
# density plot of the Sales
ggplot(CS_train, aes(x = Sales)) + geom_density()
shapiro.test(CS_train$Sales) # p-value > 0.05, accept H0, Sales seems "normal"
## 3.2
# load attached .R files
source("./create_lm_transformer.R")
source("./predict_lm_transformer.R")
# Create lm transformation plan object
cs_lm_transformer = create_lm_transformer(CS_train, "Sales")
# Convert X of training and testing dataset into datasets in new feature spaces
CS_train_t = predict_lm_transformer(CS_train, "Sales", cs_lm_transformer)
CS_train_t = as.data.frame(CS_train_t)
CS_train_t$Sales = CS_train$Sales
CS_test_t = predict_lm_transformer(CS_test, "Sales", cs_lm_transformer)
CS_test_t = as.data.frame(CS_test_t)
CS_test_t$Sales = CS_test$Sales
# fit linear models
# original data
model = lm(Sales ~., data = CS_train)
mae(CS_train$Sales, predict(model, CS_train)) # 0.8241471
mae(CS_test$Sales, predict(model, CS_test)) # 0.7933978
# transformed data
model = lm(Sales ~., data = CS_train_t)
mae(CS_train_t$Sales, predict(model, CS_train_t)) # 0.8295292
mae(CS_test_t$Sales, predict(model, CS_test_t)) # 0.7990793
## 3.3
# applying k-nearest neighbors (KNN) regression algorithm
pred = knn.reg(CS_train_t[, -20], CS_train_t[, -20], CS_train_t$Sales)
mae(pred$pred, CS_train_t$Sales) # 0.2272619
pred = knn.reg(CS_train_t[, -20], CS_test_t[, -20], CS_train_t$Sales)
mae(pred$pred, CS_test_t$Sales) # 0.2725833
rm(list=ls())
pacman::p_load(tidyverse, data.table, jsonlite, corrplot)
tw = fread("./tw.csv", fill = T, stringsAsFactors = F, encoding = "UTF-8", drop = c("title", "publishedAt", "channelId", "channelTitle", "trending_date", "tags", "thumbnail_link", "comments_disabled", "ratings_disabled", "description", "view_count"))
setwd("C:/Users/user/bigdata/巨量期末/特徵工程")
rm(list=ls())
pacman::p_load(tidyverse, data.table, jsonlite, corrplot)
tw = fread("./tw.csv", fill = T, stringsAsFactors = F, encoding = "UTF-8", drop = c("title", "publishedAt", "channelId", "channelTitle", "trending_date", "tags", "thumbnail_link", "comments_disabled", "ratings_disabled", "description", "view_count"))
View(tw)
tw_edu = tw %>% filter(categoryId == 27)
View(tw_edu)
View(tw)
View(tw_edu)
tw = fread("./tw.csv", fill = T, stringsAsFactors = F, encoding = "UTF-8", drop = c("title", "publishedAt", "channelId", "channelTitle", "trending_date", "tags", "thumbnail_link", "comments_disabled", "ratings_disabled", "description", "view_count"))
tw$likes_ratio = ifelse(tw$dislikes == 0, 0, tw$likes / tw$dislikes)
tw_edu = tw %>% filter(categoryId == 27)
View(tw_edu)
shapiro.test(tw$income)
sapply(tw_edu[, -c(1, 2)], function(x) cor.test(x, tw$income, method = "spearman")$p.value)
shapiro.test(tw$income)
sapply(tw_edu[, -c(1, 2)], function(x) cor.test(x, tw$income, method = "spearman")$p.value)
sapply(tw_edu[, -c(1, 2)], function(x) cor.test(x, tw_edu$income, method = "spearman")$p.value)
y1_pvalue = sapply(tw_edu[, -c(1, 2)], function(x) cor.test(x, tw_edu$income, method = "spearman")$p.value)
names(y1_pvalue[y1_pvalue < 0.05])
y1_pvalue = sapply(tw_edu[, -c(1, 2, 6)], function(x) cor.test(x, tw_edu$income, method = "spearman")$p.value)
names(y1_pvalue[y1_pvalue < 0.05])
tw_cor = cor(tw_edu[, -c(1, 2)])
# 與 income 高度正相關 : likes、dislikes、comment_count
# 與 likes_ration 高度正相關 : 無
corrplot(tw_cor, method = "number", type = "upper")
# setwd("~/GitHub/bigdata/巨量期末/特徵工程")
rm(list=ls())
pacman::p_load(tidyverse, data.table, jsonlite, corrplot)
# load tw.csv
tw = fread("./tw.csv", fill = T, stringsAsFactors = F, encoding = "UTF-8", drop = c("title", "publishedAt", "channelId", "channelTitle", "trending_date", "tags", "thumbnail_link", "comments_disabled", "ratings_disabled", "description", "view_count"))
# load category JSON file，讀取類別 JSON 檔
cjson = fromJSON("./TW_category_id.json"); cid = cjson$items$id; ctable = as.data.frame(cid); ctable$category = cjson$items$snippet$title
# add category name to youtube data frame，新增類別名稱欄位
tw$categoryId = as.factor(tw$categoryId)
tw = merge(tw, ctable, by.x = "categoryId", by.y = "cid")
tw$categoryId = NULL
tw$category = as.factor(tw$category)
# 新增喜歡比欄位
tw$likes_ratio = ifelse(tw$dislikes == 0, 0, tw$likes / tw$dislikes)
rm(list=c("cjson", "ctable", "cid"))
## 相關係數圖
tw_cor = cor(tw[, -c(1, 10)])
# 與 income 高度正相關 : likes、dislikes、comment_count
# 與 likes_ration 高度正相關 : 無
corrplot(tw_cor, method = "number", type = "upper")
rm(list=ls())
pacman::p_load(tidyverse, data.table, jsonlite, corrplot)
tw = fread("./tw.csv", fill = T, stringsAsFactors = F, encoding = "UTF-8", drop = c("title", "publishedAt", "channelId", "channelTitle", "trending_date", "tags", "thumbnail_link", "comments_disabled", "ratings_disabled", "description", "view_count"))
tw$likes_ratio = ifelse(tw$dislikes == 0, 0, tw$likes / tw$dislikes)
tw_edu = tw %>% filter(categoryId == 27)
shapiro.test(tw$income)
y1_pvalue = sapply(tw_edu[, -c(1, 2, 6)], function(x) cor.test(x, tw_edu$income, method = "spearman")$p.value)
names(y1_pvalue[y1_pvalue < 0.05])
# setwd("~/GitHub/bigdata/巨量期末/特徵工程")
rm(list=ls())
pacman::p_load(tidyverse, data.table, jsonlite, corrplot)
# load tw.csv
tw = fread("./tw.csv", fill = T, stringsAsFactors = F, encoding = "UTF-8", drop = c("title", "publishedAt", "channelId", "channelTitle", "trending_date", "tags", "thumbnail_link", "comments_disabled", "ratings_disabled", "description", "view_count"))
# load category JSON file，讀取類別 JSON 檔
cjson = fromJSON("./TW_category_id.json"); cid = cjson$items$id; ctable = as.data.frame(cid); ctable$category = cjson$items$snippet$title
# add category name to youtube data frame，新增類別名稱欄位
tw$categoryId = as.factor(tw$categoryId)
tw = merge(tw, ctable, by.x = "categoryId", by.y = "cid")
tw$categoryId = NULL
tw$category = as.factor(tw$category)
# 新增喜歡比欄位
tw$likes_ratio = ifelse(tw$dislikes == 0, 0, tw$likes / tw$dislikes)
rm(list=c("cjson", "ctable", "cid"))
## 相關係數圖
tw_cor = cor(tw[, -c(1, 10)])
# 與 income 高度正相關 : likes、dislikes、comment_count
# 與 likes_ration 高度正相關 : 無
corrplot(tw_cor, method = "number", type = "upper")
## 預測目標 1 : income, 預測目標 2 : likes_ratio
y1 = tw$income; tw$income = NULL
y2 = tw$likes_ratio; tw$likes_ratio = NULL
ids = tw$video_id; tw$video_id = NULL # video_id
tw$category = as.numeric(tw$category) # label encoding
## cor.test()
# income p-value < 0.05 : "likes" "dislikes" "comment_count" "desc_length" "trending_days" "duration" "category"
shapiro.test(y1) # p-value < 0.05, income is not normal
y1_pvalue = sapply(tw, function(x) cor.test(x, y1, method = "spearman", exact = F)$p.value)
names(y1_pvalue[y1_pvalue < 0.05])
# likes_ratio p-value < 0.05 : "likes" "comment_count" "desc_length" "trending_days" "duration"
shapiro.test(y2) # p-value < 0.05, likes_ratio is not normal
y2_pvalue = sapply(tw, function(x) cor.test(x, y2, method = "spearman", exact = F)$p.value)
names(y2_pvalue[y2_pvalue < 0.05])
pacman::p_load(tidyverse, data.table, jsonlite, corrplot, mltools)
# setwd("~/GitHub/bigdata/巨量期末/特徵工程")
rm(list=ls())
pacman::p_load(tidyverse, data.table, jsonlite, corrplot, mltools)
# load tw.csv
tw = fread("./tw.csv", fill = T, stringsAsFactors = F, encoding = "UTF-8", drop = c("title", "publishedAt", "channelId", "channelTitle", "trending_date", "tags", "thumbnail_link", "comments_disabled", "ratings_disabled", "description", "view_count"))
# load category JSON file，讀取類別 JSON 檔
cjson = fromJSON("./TW_category_id.json"); cid = cjson$items$id; ctable = as.data.frame(cid); ctable$category = cjson$items$snippet$title
# add category name to youtube data frame，新增類別名稱欄位
tw$categoryId = as.factor(tw$categoryId)
tw = merge(tw, ctable, by.x = "categoryId", by.y = "cid")
tw$categoryId = NULL
tw$category = as.factor(tw$category)
# 新增喜歡比欄位
tw$likes_ratio = ifelse(tw$dislikes == 0, 0, tw$likes / tw$dislikes)
rm(list=c("cjson", "ctable", "cid"))
## 相關係數圖
tw_cor = cor(tw[, -c(1, 10)])
# 與 income 高度正相關 : likes、dislikes、comment_count
# 與 likes_ration 高度正相關 : 無
corrplot(tw_cor, method = "number", type = "upper")
## 預測目標 1 : income, 預測目標 2 : likes_ratio
y1 = tw$income; tw$income = NULL
y2 = tw$likes_ratio; tw$likes_ratio = NULL
ids = tw$video_id; tw$video_id = NULL # video_id
tw$category = as.numeric(tw$category) # label encoding
## cor.test()
# income p-value < 0.05 : "likes" "dislikes" "comment_count" "desc_length" "trending_days" "duration" "category"
shapiro.test(y1) # p-value < 0.05, income is not normal
y1_pvalue = sapply(tw, function(x) cor.test(x, y1, method = "spearman", exact = F)$p.value)
names(y1_pvalue[y1_pvalue < 0.05])
# likes_ratio p-value < 0.05 : "likes" "comment_count" "desc_length" "trending_days" "duration"
shapiro.test(y2) # p-value < 0.05, likes_ratio is not normal
y2_pvalue = sapply(tw, function(x) cor.test(x, y2, method = "spearman", exact = F)$p.value)
names(y2_pvalue[y2_pvalue < 0.05])
# tw$category = as.numeric(tw$category) # label encoding
d = one_hot(as.data.table(tw))
View(d)
View(tw)
# setwd("~/GitHub/bigdata/巨量期末/特徵工程")
rm(list=ls())
pacman::p_load(tidyverse, data.table, jsonlite, corrplot, mltools)
# load tw.csv
tw = fread("./tw.csv", fill = T, stringsAsFactors = F, encoding = "UTF-8", drop = c("title", "publishedAt", "channelId", "channelTitle", "trending_date", "tags", "thumbnail_link", "comments_disabled", "ratings_disabled", "description", "view_count"))
# load category JSON file，讀取類別 JSON 檔
cjson = fromJSON("./TW_category_id.json"); cid = cjson$items$id; ctable = as.data.frame(cid); ctable$category = cjson$items$snippet$title
# add category name to youtube data frame，新增類別名稱欄位
tw$categoryId = as.factor(tw$categoryId)
tw = merge(tw, ctable, by.x = "categoryId", by.y = "cid")
tw$categoryId = NULL
tw$category = as.factor(tw$category)
# 新增喜歡比欄位
tw$likes_ratio = ifelse(tw$dislikes == 0, 0, tw$likes / tw$dislikes)
rm(list=c("cjson", "ctable", "cid"))
## 相關係數圖
tw_cor = cor(tw[, -c(1, 10)])
# 與 income 高度正相關 : likes、dislikes、comment_count
# 與 likes_ration 高度正相關 : 無
corrplot(tw_cor, method = "number", type = "upper")
## 預測目標 1 : income, 預測目標 2 : likes_ratio
y1 = tw$income; tw$income = NULL
y2 = tw$likes_ratio; tw$likes_ratio = NULL
ids = tw$video_id; tw$video_id = NULL # video_id
# tw$category = as.numeric(tw$category) # label encoding
d = one_hot(as.data.table(tw))
View(d)
View(d)
# setwd("~/GitHub/bigdata/巨量期末/特徵工程")
rm(list=ls())
pacman::p_load(tidyverse, data.table, jsonlite, corrplot, mltools)
# load tw.csv
tw = fread("./tw.csv", fill = T, stringsAsFactors = F, encoding = "UTF-8", drop = c("title", "publishedAt", "channelId", "channelTitle", "trending_date", "tags", "thumbnail_link", "comments_disabled", "ratings_disabled", "description", "view_count"))
# load category JSON file，讀取類別 JSON 檔
cjson = fromJSON("./TW_category_id.json"); cid = cjson$items$id; ctable = as.data.frame(cid); ctable$category = cjson$items$snippet$title
# add category name to youtube data frame，新增類別名稱欄位
tw$categoryId = as.factor(tw$categoryId)
tw = merge(tw, ctable, by.x = "categoryId", by.y = "cid")
tw$categoryId = NULL
tw$category = as.factor(tw$category)
# 新增喜歡比欄位
tw$likes_ratio = ifelse(tw$dislikes == 0, 0, tw$likes / tw$dislikes)
rm(list=c("cjson", "ctable", "cid"))
## 相關係數圖
tw_cor = cor(tw[, -c(1, 10)])
# 與 income 高度正相關 : likes、dislikes、comment_count
# 與 likes_ration 高度正相關 : 無
corrplot(tw_cor, method = "number", type = "upper")
## 預測目標 1 : income, 預測目標 2 : likes_ratio
y1 = tw$income; tw$income = NULL
y2 = tw$likes_ratio; tw$likes_ratio = NULL
ids = tw$video_id; tw$video_id = NULL # video_id
# tw$category = as.numeric(tw$category) # label encoding
tw = one_hot(as.data.table(tw))
## cor.test()
# income p-value < 0.05 : "likes" "dislikes" "comment_count" "desc_length" "trending_days" "duration" "category"
shapiro.test(y1) # p-value < 0.05, income is not normal
y1_pvalue = sapply(tw, function(x) cor.test(x, y1, method = "spearman", exact = F)$p.value)
names(y1_pvalue[y1_pvalue < 0.05])
# likes_ratio p-value < 0.05 : "likes" "comment_count" "desc_length" "trending_days" "duration"
shapiro.test(y2) # p-value < 0.05, likes_ratio is not normal
y2_pvalue = sapply(tw, function(x) cor.test(x, y2, method = "spearman", exact = F)$p.value)
names(y2_pvalue[y2_pvalue < 0.05])
# Min-max scaling
normalize = function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
tw_nor = as.data.frame(lapply(tw, normalize))
tw_nor$income = y1
tw_nor$likes_ratio = y2
tw_nor$video_id = ids
tw_nor = tw_nor[c(11, 1:10)]
## output new_tw2.csv (after scaling)
write.table(tw_nor, "./new_tw2.csv", sep = ",", row.names = F)
View(tw)
View(tw_nor)
tw_nor = as.data.frame(lapply(tw, normalize))
View(tw_nor)
# setwd("~/GitHub/bigdata/巨量期末/特徵工程")
rm(list=ls())
pacman::p_load(tidyverse, data.table, jsonlite, corrplot, mltools)
a = fread("./new_tw2.csv")
View(a)
# setwd("~/GitHub/bigdata/巨量期末/特徵工程")
rm(list=ls())
pacman::p_load(tidyverse, data.table, jsonlite, corrplot, mltools)
# load tw.csv
tw = fread("./tw.csv", fill = T, stringsAsFactors = F, encoding = "UTF-8", drop = c("title", "publishedAt", "channelId", "channelTitle", "trending_date", "tags", "thumbnail_link", "comments_disabled", "ratings_disabled", "description", "view_count"))
# load category JSON file，讀取類別 JSON 檔
cjson = fromJSON("./TW_category_id.json"); cid = cjson$items$id; ctable = as.data.frame(cid); ctable$category = cjson$items$snippet$title
# add category name to youtube data frame，新增類別名稱欄位
tw$categoryId = as.factor(tw$categoryId)
tw = merge(tw, ctable, by.x = "categoryId", by.y = "cid")
tw$categoryId = NULL
tw$category = as.factor(tw$category)
# 新增喜歡比欄位
tw$likes_ratio = ifelse(tw$dislikes == 0, 0, tw$likes / tw$dislikes)
rm(list=c("cjson", "ctable", "cid"))
## 相關係數圖
tw_cor = cor(tw[, -c(1, 10)])
# 與 income 高度正相關 : likes、dislikes、comment_count
# 與 likes_ration 高度正相關 : 無
corrplot(tw_cor, method = "number", type = "upper")
## 預測目標 1 : income, 預測目標 2 : likes_ratio
y1 = tw$income; tw$income = NULL
y2 = tw$likes_ratio; tw$likes_ratio = NULL
ids = tw$video_id; tw$video_id = NULL # video_id
# tw$category = as.numeric(tw$category) # label encoding
tw = one_hot(as.data.table(tw))
## cor.test()
# income p-value < 0.05 : "likes" "dislikes" "comment_count" "desc_length" "trending_days" "duration" "category"
shapiro.test(y1) # p-value < 0.05, income is not normal
y1_pvalue = sapply(tw, function(x) cor.test(x, y1, method = "spearman", exact = F)$p.value)
names(y1_pvalue[y1_pvalue < 0.05])
# likes_ratio p-value < 0.05 : "likes" "comment_count" "desc_length" "trending_days" "duration"
shapiro.test(y2) # p-value < 0.05, likes_ratio is not normal
y2_pvalue = sapply(tw, function(x) cor.test(x, y2, method = "spearman", exact = F)$p.value)
names(y2_pvalue[y2_pvalue < 0.05])
# Min-max scaling
normalize = function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
tw_nor = as.data.frame(lapply(tw, normalize))
tw_nor$income = y1
tw_nor$likes_ratio = y2
tw_nor$video_id = ids
View(tw_nor)
source('C:/Users/user/bigdata/巨量期末/特徵工程/Feature_enginerring.R', encoding = 'UTF-8', echo=TRUE)
tw_nor = tw_nor[c(24, 1:23)]
View(tw_nor)
# Min-max scaling
normalize = function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
tw_nor = as.data.frame(lapply(tw, normalize))
tw_nor$income = y1
tw_nor$likes_ratio = y2
tw_nor$video_id = ids
tw_nor = tw_nor[c(24, 1:23)]
View(tw_nor)
## output new_tw2.csv (after scaling)
write.table(tw_nor, "./new_tw2.csv", sep = ",", row.names = F)
a = fread("./new_tw2.csv")
View(a)

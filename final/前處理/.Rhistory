setwd("C:/Users/Gary/Downloads/final前處理修改/final前處理修改")
tw = fread("./tw.csv")
rm(list = ls())
pacman::p_load(data.table, tidyverse)
tw = fread("./tw.csv")
us = fread("./us.csv")
View(tw)
unique(tw$video_id)
group_by(tw, video_id) %>% summarise(n = n()) %>% arrange(desc(n))
sapply(tw, function(x) sum(ifelse(is.na(x), 1, 0)))
rm(list = ls())
pacman::p_load(data.table, tidyverse)
tw = fread("./tw.csv")
write.csv(tw_duration,file="tw.csv",row.names = F, fileEncoding = "UTF-8")
tw_duration<- dplyr::inner_join(tw_mostViews,tw_Time,by="video_id")
#新增每部影片上幾天熱門的欄位
tw = group_by(tw, video_id) %>% mutate(trending_days = n())
#將每部影片最終的資料獨立出來，避免重複統計。
tw_mostViews = group_by(tw, video_id) %>% filter(view_count == max(view_count))
#把爬到台灣資料 影片長度json檔導入
tw_id_json<- fromJSON("R dataset/final/scrape/tw_id.json")
tw_time_json<- fromJSON("R dataset/final/scrape/tw_numeric_times.json")
#把影片長度合併到CSV
tw_Time<-data.frame(video_id=(tw_id_json),duration=(tw_time_json))
rm(list = ls())
pacman::p_load(data.table, tidyverse)
tw = fread("./tw.csv")
write.csv(tw ,file="tw1.csv",row.names = F, fileEncoding = "UTF-8")
rm(list=ls()) # clean env
pacman::p_load("data.table", "tidyverse", "sqldf", "jsonlite", "corrplot", "d3heatmap") # load packages
yt = read_csv("./USvideos.csv") # read csv
setwd("~/巨量資料分析/巨量期中")
yt = read_csv("./USvideos.csv") # read csv
View(yt)
us = fread("./us.csv")
setwd("~/GitHub/bigdata_team_project/巨量期末/前處理")
us = fread("./us.csv")
write.csv(us ,file="us1.csv",row.names = F, fileEncoding = "UTF-8")
tw = fread("./tw.csv")
category_cor = cor(tw[, -1])
corrplot(category_cor, method="number", type="upper")
category_cor = cor(tw[, -1])
View(tw)
category_cor = cor(tw[, c(9:12)])
corrplot(category_cor, method="number", type="upper")
